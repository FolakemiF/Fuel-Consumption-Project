{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b39916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "data = pd.read_csv('MY2010-2014 Fuel Consumption Ratings 5-cycle.csv', encoding='latin1', low_memory=False)\n",
    "#copy daata\n",
    "df = data.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0897bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1b848",
   "metadata": {},
   "source": [
    "### observation\n",
    "- Empty columns from coloumn 15\n",
    "- row 0 is part of the coloumn name\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14b948",
   "metadata": {},
   "source": [
    "### Data Cleaning and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(df.columns):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop multiple columns by slicing\n",
    "df.drop(df.columns[13:], axis=1, inplace=True)\n",
    "\n",
    "#reference : https://www.geeksforgeeks.org/how-to-drop-one-or-multiple-columns-in-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04645952",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(df.columns):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename variables\n",
    "df = df.rename(columns={\"MODEL\":\"MODEL YEAR\",\"MODEL.1\":\"MODEL\", \"ENGINE SIZE\":\"ENGINE SIZE(L)\",\"FUEL\":\"FUEL TYPE\", \"FUEL CONSUMPTION*\":\"FC CITY (L/100 km)\",\n",
    "                        \"Unnamed: 9\":\"FC HWY (L/100 km)\", \"Unnamed: 10\": \"FC COMBINED(L/100 km)\",\n",
    "                        \"Unnamed: 11\":\"FC COMBINED(mpg)\",\"CO2 EMISSIONS \":\"CO2 EMISSIONS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d232c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove row 0\n",
    "df = df.drop([0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559dfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "df.reset_index(drop=True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61034c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe65133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa7f24",
   "metadata": {},
   "source": [
    "### observations\n",
    "- variables that are meant to be float is showing as object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b28a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicated\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "#all coloumns have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MODEL YEAR\"].unique()\n",
    "#error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed83ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a629026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#issues from row 5360\n",
    "#drop all rows from 5369\n",
    "df.drop(df.index[5359:], inplace=True)\n",
    "\n",
    "#reference: https://stackoverflow.com/questions/14661701/how-to-drop-a-list-of-rows-from-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MODEL YEAR\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488df18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MAKE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MODEL\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"VEHICLE CLASS\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ENGINE SIZE(L)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b679c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float\n",
    "df[\"ENGINE SIZE(L)\"]= df[\"ENGINE SIZE(L)\"].astype('float')\n",
    "df[\"ENGINE SIZE(L)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CYLINDERS\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48dc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TRANSMISSION\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666adfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= df.replace({'TRANSMISSION':{'AS.*' : 'AS', 'AM.*' : 'AM', 'AV.*':'AV','M5':'M','M6':'M','M7':'M', 'A6':'A', 'A8':'A', 'A7':'A', 'A4':'A', 'A5':'A', 'A9':'A'} }, regex = True)                                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"TRANSMISSION\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FUEL TYPE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FC CITY (L/100 km)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float\n",
    "df[\"FC CITY (L/100 km)\"]= df[\"FC CITY (L/100 km)\"].astype('float')\n",
    "df[\"FC CITY (L/100 km)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bcf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FC HWY (L/100 km)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float\n",
    "df[\"FC HWY (L/100 km)\"]= df[\"FC HWY (L/100 km)\"].astype('float')\n",
    "df[\"FC HWY (L/100 km)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FC COMBINED(L/100 km)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64212ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float\n",
    "df[\"FC COMBINED(L/100 km)\"]= df[\"FC COMBINED(L/100 km)\"].astype('float')\n",
    "df[\"FC COMBINED(L/100 km)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62597001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FC COMBINED(mpg)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FC COMBINED(mpg)\"]= df[\"FC COMBINED(mpg)\"].astype('int')\n",
    "df[\"FC COMBINED(mpg)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8877708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CO2 EMISSIONS\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CO2 EMISSIONS\"]= df[\"CO2 EMISSIONS\"].astype('int')\n",
    "df[\"CO2 EMISSIONS\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9be211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7045c",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check distribution\n",
    "plt.figure(dpi=200);\n",
    "df.hist(figsize = (20,15));\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f4146",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The distribution for CYLINDERS appears to be descrete.\n",
    "The distribution for FUELCONSUMPTION_CITY, FUELCONSUMPTION_HWY and FUELCONSUMPTION_COMB appears to be right skewed. This can also be confirmed by looking at the mean\n",
    "and median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check fo ouliers\n",
    "df.boxplot(figsize = (15,10), fontsize = 15, rot = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3e1df",
   "metadata": {},
   "source": [
    "Observations\n",
    "- There are outliers in the data that needs to be treated\n",
    "- Some variable exerts dominace over others and this will need to be standardixe or normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec98e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_outliers(df, col):\n",
    "    \"\"\"\n",
    "    treats outliers in a varaible\n",
    "    col: str, name of the numerical varaible\n",
    "    df: data frame\n",
    "    col: name of the column\n",
    "    \"\"\"\n",
    "    Q1 = df[col].quantile(0.25)  # 25th quantile\n",
    "    Q3 = df[col].quantile(0.75)  # 75th quantile\n",
    "    IQR = Q3 - Q1\n",
    "    Lower_Whisker = Q1 - 1.5 * IQR\n",
    "    Upper_Whisker = Q3 + 1.5 * IQR\n",
    "    df[col] = np.clip(\n",
    "        df[col], Lower_Whisker, Upper_Whisker\n",
    "    )  # all the values samller than Lower_Whisker will be assigned value of Lower_whisker\n",
    "    # and all the values above upper_whisker will be assigned value of upper_Whisker\n",
    "    return df\n",
    "\n",
    "\n",
    "def treat_outliers_all(df, col_list):\n",
    "    \"\"\"\n",
    "    treat outlier in all numerical varaibles\n",
    "    col_list: list of numerical varaibles\n",
    "    df: data frame\n",
    "    \"\"\"\n",
    "    for c in col_list:\n",
    "        df = treat_outliers(df, c)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc0264",
   "metadata": {},
   "source": [
    "### UNIVARIATE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dependent and independent variables\n",
    "df_num = df.select_dtypes(exclude ='object') # obtains the numerical data ty\n",
    "df_num = treat_outliers_all(df_num, df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84488bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_df = df.select_dtypes(include ='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bd151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if outliers has ben treated\n",
    "df_num.boxplot(figsize = (15,10), fontsize = 15, rot = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for coreelation\n",
    "plt.figure(figsize=(10,5))\n",
    "corr_mat = df.corr()\n",
    "sns.heatmap(data=corr_mat, annot = True, vmin=-1, cmap = \"PiYG\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b490f2",
   "metadata": {},
   "source": [
    " ### Linear Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101742e",
   "metadata": {},
   "source": [
    "- We want to predict the C02 Emission.\n",
    "\n",
    "- We'll be using all numerical continous variable\n",
    "\n",
    "- We'll split the data into train and test to be able to evaluate the model that we build on the train data.\n",
    "\n",
    "- We will build a Linear Regression model using the train data and then check it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_num.drop(['CO2 EMISSIONS'], axis= 1)\n",
    "Y = df_num['CO2 EMISSIONS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f86e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train, test and validation()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1) #traintest split\n",
    "X,Y=X_train,Y_train\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=1) #train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in test data =\", X_test.shape[0])\n",
    "print(\"Number of rows in train data =\", X_train.shape[0])\n",
    "print(\"Number of rows in validation data =\", X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57585ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e30b1e",
   "metadata": {},
   "source": [
    "### Scaling the data\n",
    "- The two most common scaller is minmax scaler and  standard scaler\n",
    "- since this is a linear regression problem. Standardiztion will be the best option\n",
    "\n",
    "- Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.\n",
    "\n",
    "- Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis\n",
    "\n",
    "#ref:https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff#:~:text=Normalization%20is%20useful%20when%20your,Gaussian%20(bell%20curve)%20distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d11eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b92c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit training data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg_model = LinearRegression()\n",
    "lin_reg_model.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1689c5c",
   "metadata": {},
   "source": [
    "### Check Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute adjusted R-squared\n",
    "def adj_r2_score(predictors, targets, predictions):\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    n = predictors.shape[0]\n",
    "    k = predictors.shape[1]\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "\n",
    "# # function to compute MAPE\n",
    "# def mape_score(targets, predictions):\n",
    "#     return np.mean(np.abs(targets - predictions) / targets) * 100\n",
    "\n",
    "\n",
    "# function to compute different metrics to check performance of a regression model\n",
    "def model_performance_regression(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check regression model performance\n",
    "\n",
    "    model: regressor\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    r2 = r2_score(target, pred)  # to compute R-squared\n",
    "    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n",
    "    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n",
    "    mae = mean_absolute_error(target, pred)  # to compute MAE\n",
    "#     mape = mape_score(target, pred)  # to compute MAPE\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R-squared\": r2,\n",
    "            \"Adj. R-squared\": adjr2\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check model performance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model_perf_train = model_performance_regression(lin_reg_model, X_train_scaled, Y_train)\n",
    "lin_reg_model_perf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e90ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking model performance on test set\n",
    "lin_reg_model_perf_test = model_performance_regression(lin_reg_model, X_test_scaled, Y_test)\n",
    "lin_reg_model_perf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1731b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model_perf_val = model_performance_regression(lin_reg_model, X_val_scaled, Y_val)\n",
    "lin_reg_model_perf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training performance')\n",
    "print(lin_reg_model_perf_train)\n",
    "print('')\n",
    "print('Test performance')\n",
    "print(lin_reg_model_perf_test)\n",
    "print('')\n",
    "print('validation performance')\n",
    "print(lin_reg_model_perf_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16357494",
   "metadata": {},
   "source": [
    "#### Performance Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166563e",
   "metadata": {},
   "source": [
    "The model underfits i.e not performing well\n",
    "\n",
    "There major reason for this might be because the model isnt complex enough\n",
    "\n",
    "will be adding more variables to make the model more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder() # declares an instance of the object\n",
    "ohe_dat=ohe.fit_transform(object_df[['FUEL TYPE']]).toarray() # applies the object to data\n",
    "feature_labels = ohe.categories_ # the labels are stored here\n",
    "feature_labels = np.array(feature_labels).ravel()\n",
    "df_ohe = pd.DataFrame(ohe_dat, columns = feature_labels ) # creating a dataframe for the ohe variable\n",
    "df_ohe_new = pd.concat([df_num,df_ohe], axis = 1) # combining the original dataframe (df) and df_ohe\n",
    "df_ohe_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "corr_mat = df_ohe_new.corr()\n",
    "sns.heatmap(data=corr_mat, annot = True, vmin=-1, cmap = \"PiYG\");\n",
    "#plt.subplots_adjust(right=0.8, left=0.2)\n",
    "plt.savefig('Correlation Fuel type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509753d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_ohe_new.drop(['D'], axis = 1)\n",
    "#very low correclation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df2.drop(['CO2 EMISSIONS'], axis= 1)\n",
    "Y2 = df2['CO2 EMISSIONS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train, test and validation()\n",
    "\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size=0.2, random_state=1) #traintest split\n",
    "X2,Y2=X2_train,Y2_train\n",
    "\n",
    "X2_train, X2_val, Y2_train, Y2_val = train_test_split(X2, Y2, test_size=0.2, random_state=1) #train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7faf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit training data\n",
    "#standardize\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X2_train_scaled = scaler.fit_transform(X2_train)\n",
    "X2_test_scaled = scaler.transform(X2_test)\n",
    "X2_val_scaled = scaler.transform(X2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05dd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model2 = LinearRegression()\n",
    "lin_reg_model2.fit(X2_train_scaled, Y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31219ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model_perf_train = model_performance_regression(lin_reg_model2, X2_train_scaled, Y2_train)\n",
    "lin_reg_model_perf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model_perf_test = model_performance_regression(lin_reg_model2, X2_test_scaled, Y2_test)\n",
    "lin_reg_model_perf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model_perf_val = model_performance_regression(lin_reg_model2, X2_val_scaled, Y2_val)\n",
    "lin_reg_model_perf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training performance')\n",
    "print(lin_reg_model_perf_train)\n",
    "print('')\n",
    "print('Test performance')\n",
    "print(lin_reg_model_perf_test)\n",
    "print('')\n",
    "print('validation performance')\n",
    "print(lin_reg_model_perf_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace0619",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c055fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['MODEL YEAR']= df['MODEL YEAR'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x ='MODEL YEAR', y='CO2 EMISSIONS');\n",
    "plt.title('CO2 Emission by Year');\n",
    "plt.savefig('Yearly CO2 Emissions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04548df",
   "metadata": {},
   "source": [
    "#### There's constant reduction in fuel consumption from 2012 to 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904be5c",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1848c2d",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC # SVC: support vector classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (accuracy_score,precision_score, recall_score,f1_score,precision_recall_curve,\n",
    "                             classification_report,confusion_matrix, ConfusionMatrixDisplay, roc_curve,roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending several models for cross validation\n",
    "models = []\n",
    "models.append((\"LR\",LogisticRegression()))\n",
    "models.append((\"KNN\",KNeighborsClassifier()))\n",
    "models.append((\"Dtree\",DecisionTreeClassifier()))\n",
    "models.append((\"SVC\",SVC()))\n",
    "models.append((\"RF\",RandomForestClassifier()))\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca708fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_cross_val_score(models):\n",
    "    for name,model in models:\n",
    "        kfold = KFold(n_splits=5)\n",
    "        cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "        #print(name, cv_result.mean())\n",
    "        print(name, cv_result)\n",
    "\n",
    "\n",
    "report_cross_val_score(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model test report\n",
    "def report_model_test(models):\n",
    "    for name,model in models:\n",
    "        model.fit(X_train_scaled,Y_train)\n",
    "        basem_preds = model.predict(X_test_scaled)\n",
    "        print(name)\n",
    "        print(classification_report(Y_test,basem_preds))\n",
    "        ConfusionMatrixDisplay.from_estimator(model, X_test_scaled, Y_test)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(40,40), dpi = 200);\n",
    "\n",
    "\n",
    "report_model_test(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1814cc",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e06dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caaaac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()# declares an instance of the object\n",
    "le_data = object_df.apply(le.fit_transform)# applies the object to data\n",
    "df_le = pd.DataFrame(le_data) # creates a dataframe\n",
    "new_df_le = pd.concat([df_num,df_le], axis = 1)\n",
    "new_df_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48069d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "corr_mat = new_df_le.corr()\n",
    "plt.subplots_adjust(bottom = 0.3)\n",
    "sns.heatmap(data=corr_mat, annot = True, vmin=-1, cmap = \"PiYG\");\n",
    "plt.savefig(' All Corr Matt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d604d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_le =new_df_le.drop(['MODEL YEAR', 'MODEL'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "corr_mat = new_df_le.corr()\n",
    "sns.heatmap(data=corr_mat, annot = True, vmin=-1, cmap = \"PiYG\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_model(model, predictor, target):\n",
    "    preds = model.predict(predictor)\n",
    "\n",
    "    feature_importance=pd.DataFrame(data=model.feature_importances_, index =predictor.columns,\n",
    "    columns = ['Feature_importance']).sort_values('Feature_importance')\n",
    "    print(feature_importance)\n",
    "    print(classification_report(target,preds))\n",
    "    plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "    ConfusionMatrixDisplay.from_estimator(model,predictor, target)\n",
    "\n",
    "    plt.show()\n",
    "    plot_tree(model, feature_names = predictor.columns, filled = True, fontsize=20 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_position=plt.subplots(1,2,figsize=(15,6),dpi=200) # creates the framework for the plotting\n",
    "a = sns.countplot(x='MAKE', data=new_df_le, ax=ax_position[0]) #ax_position[0] specifies plot to be in index 0\n",
    "a= df['MAKE'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax_position[1]) #ax_position[1] specifies plot to be in index 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using 'MAKE' as target\n",
    "x = new_df_le.drop(['MAKE','VEHICLE CLASS','FUEL TYPE'], axis=1)\n",
    "y = new_df_le['MAKE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = RandomOverSampler(random_state=42) # The object is created\n",
    "x_res, y_res = sm.fit_resample(x, y) # The object is applied\n",
    "x, y = x_res, y_res # reassigning the balanced dataset to X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c07317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the dataset\n",
    "df_bal = pd.concat([x_res,y_res], axis = 1) # creating a dataframe for the balanced data\n",
    "fig, ax=plt.subplots(1,2,figsize=(15,6)) # creating the axis shell for subplot\n",
    "a = sns.countplot(x='MAKE',data=df_bal, ax=ax[0]) # assigning each of the plot to the axis shell\n",
    "a= df_bal['MAKE'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1]) # assigning each of the plot to the axis shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train, test and validation()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1) #traintest split\n",
    "x,y=x_train,y_train\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1) #train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769333d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6259490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a941b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_position=plt.subplots(1,2,figsize=(15,6),dpi=200) # creates the framework for the plotting\n",
    "a = sns.countplot(x='VEHICLE CLASS', data=new_df_le, ax=ax_position[0]) #ax_position[0] specifies plot to be in index 0\n",
    "a= df['VEHICLE CLASS'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax_position[1]) #ax_position[1] specifies plot to be in index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using VEHICLE CLASS as target\n",
    "x2 = new_df_le.drop(['TRANSMISSION', 'CYLINDERS', 'ENGINE SIZE(L)', 'VEHICLE CLASS', 'FUEL TYPE', 'MAKE'], axis=1)\n",
    "y2= new_df_le['VEHICLE CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa87e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = RandomOverSampler(random_state=42) # The object is created\n",
    "x2_res, y2_res = sm.fit_resample(x2, y2) # The object is applied\n",
    "x2, y2 = x2_res, y2_res # reassigning the balanced dataset to X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78429a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the dataset\n",
    "df_bal = pd.concat([x2_res,y2_res], axis = 1) # creating a dataframe for the balanced data\n",
    "fig, ax=plt.subplots(1,2,figsize=(15,6)) # creating the axis shell for subplot\n",
    "a = sns.countplot(x='VEHICLE CLASS',data=df_bal, ax=ax[0]) # assigning each of the plot to the axis shell\n",
    "a= df_bal['VEHICLE CLASS'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1]) # assigning each of the plot to the axis shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train, test and validation()\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.2, random_state=1) #traintest split\n",
    "x2,y2=x2_train,y2_train\n",
    "\n",
    "x2_train, x2_val, y2_train, y2_val = train_test_split(x2, y2, test_size=0.2, random_state=1) #train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37490c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a8203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1b13b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cd739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x2_val, y2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_position=plt.subplots(1,2,figsize=(15,6),dpi=200) # creates the framework for the plotting\n",
    "a = sns.countplot(x='TRANSMISSION', data=new_df_le, ax=ax_position[0]) #ax_position[0] specifies plot to be in index 0\n",
    "a= df['TRANSMISSION'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax_position[1]) #ax_position[1] specifies plot to be in index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be893f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using TRANSMISION as taerget\n",
    "x3 = new_df_le.drop(['TRANSMISSION', 'VEHICLE CLASS'], axis=1)\n",
    "y3= new_df_le['TRANSMISSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = RandomOverSampler(random_state=42) # The object is created\n",
    "x3_res, y3_res = sm.fit_resample(x3, y3) # The object is applied\n",
    "x3, y3 = x3_res, y3_res # reassigning the balanced dataset to X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the dataset\n",
    "df_bal = pd.concat([x3_res,y3_res], axis = 1) # creating a dataframe for the balanced data\n",
    "fig, ax=plt.subplots(1,2,figsize=(15,6)) # creating the axis shell for subplot\n",
    "a = sns.countplot(x='TRANSMISSION',data=df_bal, ax=ax[0]) # assigning each of the plot to the axis shell\n",
    "a= df_bal['TRANSMISSION'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1]) # assigning each of the plot to the axis shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f957eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train, test and validation()\n",
    "x3_train, x3_test, y3_train, y3_test = train_test_split(x3, y3, test_size=0.2, random_state=1) #traintest split\n",
    "x3,y3=x3_train,y3_train\n",
    "\n",
    "x3_train, x3_val, y3_train, y3_val = train_test_split(x3, y3, test_size=0.2, random_state=1) #train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x3_train, y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df4f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x3_train, y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca3c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x3_test, y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a80709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x3_val, y3_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e24c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_position=plt.subplots(1,2,figsize=(15,6),dpi=200) # creates the framework for the plotting\n",
    "a = sns.countplot(x='FUEL TYPE', data=new_df_le, ax=ax_position[0]) #ax_position[0] specifies plot to be in index 0\n",
    "a= df['FUEL TYPE'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax_position[1]) #ax_position[1] specifies plot to be in index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22207075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using FUEL TYPE as target\n",
    "x4 = new_df_le[['TRANSMISSION', 'FC HWY (L/100 km)']]\n",
    "y4= new_df_le['FUEL TYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c47474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = RandomOverSampler(random_state=42) # The object is created\n",
    "x4_res, y4_res = sm.fit_resample(x4, y4) # The object is applied\n",
    "x4, y4 = x4_res, y4_res # reassigning the balanced dataset to X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ec38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the dataset\n",
    "df_bal = pd.concat([x4_res,y4_res], axis = 1) # creating a dataframe for the balanced data\n",
    "fig, ax=plt.subplots(1,2,figsize=(15,6)) # creating the axis shell for subplot\n",
    "a = sns.countplot(x='FUEL TYPE',data=df_bal, ax=ax[0]) # assigning each of the plot to the axis shell\n",
    "a= df_bal['FUEL TYPE'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1]) # assigning each of the plot to the axis shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ef77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split into train, test and validation()\n",
    "x4_train, x4_test, y4_train, y4_test = train_test_split(x4, y4, test_size=0.2, random_state=1) #traintest split\n",
    "x4,y4=x4_train,y4_train\n",
    "\n",
    "x4_train, x4_val, y4_train, y4_val = train_test_split(x4, y4, test_size=0.2, random_state=1) #train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x4_train, y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60513b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x4_train, y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d5516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x4_test, y4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe146407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_model(model, x4_val, y4_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0182a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c085765",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the attributes to see how they affect the clusters formed\n",
    "for col1 in df_num.columns:\n",
    "    for col2 in df_num.columns:\n",
    "        plt.figure(figsize = (8,9),dpi = 200)\n",
    "        sns.set(font_scale=2)\n",
    "        sns.scatterplot(data = df_num, x = df_num[col1],y = df_num[col2],\n",
    "        hue='CYLINDERS', palette = 'viridis')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37030ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histogram to see their relationship with categories\n",
    "for col in df_num.columns:\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize=(20,15), dpi = 200)\n",
    "    sns.histplot(data=df,x = col, hue = 'CYLINDERS', palette = 'viridis')\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the count of the values to see the useful categories\n",
    "for col in object_df.columns:\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize=(20,15), dpi = 200)\n",
    "    sns.countplot(data = object_df,x=col, order = object_df[col].value_counts().index)\n",
    "    plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encodeded caterogical data\n",
    "df_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop model year and model as it has little effect to the data\n",
    "obj_df = df_le.drop(['MODEL YEAR'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcad264",
   "metadata": {},
   "source": [
    "#### Scaling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5de8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_num_df=scaler.fit_transform(df_num)\n",
    "scaled_num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2020c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_obj_df=scaler.fit_transform(obj_df)\n",
    "scaled_obj_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57a2a7",
   "metadata": {},
   "source": [
    "#### Kmeans Clusttering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef067deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(scaled_num_df)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "#Visualing the plot\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.savefig('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecca0da",
   "metadata": {},
   "source": [
    "#### Elbow curve is at 2 which means optimal value of k is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77acc9",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecf1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # import KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f246f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, random_state = 0) # assigning the number of clusters to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = model.fit_predict(scaled_num_df) # fiting and predicting the clusters using numerical variabies\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a19880",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df['CLUSTER']=cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae470a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj_df.drop('CLUSTER', axis = 1, inplace = True)\n",
    "for col1 in obj_df.columns:\n",
    "    for col2 in obj_df.columns:\n",
    "        plt.figure(figsize = (8,9),dpi = 200)\n",
    "        sns.set(font_scale=2)\n",
    "        sns.scatterplot(data = obj_df, x = obj_df[col1],y = obj_df[col2],\n",
    "        hue=cluster_labels, palette = 'viridis')\n",
    "        plt.savefig('all clus')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83916ff9",
   "metadata": {},
   "source": [
    "### Clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23645985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Internal Evaluation\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fec905",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_index=davies_bouldin_score(scaled_obj_df,cluster_labels) #df_le = Encoded Cateogircal varialbes\n",
    "DB_index                                            #cluster_label = only numerical variables used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e010ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score=silhouette_score(scaled_obj_df,cluster_labels)\n",
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering #here we import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbf863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(n_clusters=7,linkage='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels=model.fit_predict(scaled_num_df)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dc206",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_index=davies_bouldin_score(scaled_obj_df,cluster_labels)\n",
    "DB_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf38746",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score=silhouette_score(scaled_obj_df,cluster_labels)\n",
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(n_clusters=None, distance_threshold=0) # distance_threshold=0 means all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels=model.fit_predict(scaled_num_df)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy # scipy is used for plotting the hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da607950",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_linkage = hierarchy.linkage(model.children_) # hierarchy is used to obtain the children\n",
    "matrix_linkage # distances between the points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2696ecd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12), dpi = 200,)\n",
    "dendrogram(Z=matrix_linkage,truncate_mode= 'level', p= 7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccffdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
